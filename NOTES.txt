

1. Introduction

Data Structures: to store data in an efficient way. When we use the most proper data structures for the algorithm it boost up running speed of that. That is why we might need different data structures.

ADT (Abstract Data Type) is just spesfification or model (logical description) of certain data structures. ADT does not spesifies implementation it just define what methods so data structure will have, so 
we define the basic behavior. ADT is language independent. For example, Queue is ADT and should have push(), pop() and peek() methods.

On the other side "Data Structures" are concrete implementation of ADT. For example ArrayList, Queue, HashMap, Array is a some the data structure in Java. 
(Note sometimes ADT and Data Structres are having same name but dont mix them)

1. ARRAYS

Is a collection of elements each identified by an array index. Index starts from 0. 
Array is data structure in order to store collection of data with same type. Array can have as many dimensien as we want. 

PROS
1) We can use random access because of its key. array[4]. (O(1) time complexity)
2) Very easy to implement and use.
3) Very fast.


CONS
1) We have to know size of array in compile time.
2) If it is full in order to have more items inside ti we should create new nigger array and move all of its value inot there whihc will take O(N) time complexity.
3) It is not able to store data with different types



ARRAY OPERATIONS AND THEIR TIME COMPLEXITIES
1) Adding new item to end of array. (Of course if array is not full). O(1).
2) Inserting item into spesific index. (Of course if array is not full) O(N) in the worst scenario.

3) Removing last item of array. O(1).
4) Removing item by array index. O(N). in the worst scenario.

5) Retriving data by its index. O(1)
6) Searching value by its value. O(N). Because we should iterate array.


ArrayList is resizable-array implementation of List interface in java. It permtis all elements included null. (This class is roughly equivalent to Vector, except that it is unsynchronized.)

The size, isEmpty, get, set, iterator, and listIterator operations run in constant time. The add operation runs in amortized constant time, that is, adding n elements requires O(n) time. 
All of the other operations run in linear time (roughly speaking). 

Each ArrayList instance has a capacity. The capacity is the size of the array used to store the elements in the list. It is always at least as large as the list size. As elements are added 
to an ArrayList, its capacity grows automatically. The details of the growth policy are not specified beyond the fact that adding an element has constant amortized time cost.

An application can increase the capacity of an ArrayList instance before adding a large number of elements using the ensureCapacity operation. This may reduce the amount of incremental reallocation.

Note that this implementation is not synchronized. If multiple threads access an ArrayList instance concurrently, and at least one of the threads modifies the list structurally, it must be 
synchronized externally. (A structural modification is any operation that adds or deletes one or more elements, or explicitly resizes the backing array; merely setting the value of an element 
is not a structural modification.) This is typically accomplished by synchronizing on some object that naturally encapsulates the list. If no such object exists, the list should be "wrapped" using 
the Collections.synchronizedList method. This is best done at creation time, to prevent accidental unsynchronized access to the list:

   List list = Collections.synchronizedList(new ArrayList(...));

The iterators returned by this class's iterator and listIterator methods are fail-fast: if the list is structurally modified at any time after the iterator is created, in any way except through the 
iterator's own remove or add methods, the iterator will throw a ConcurrentModificationException. Thus, in the face of concurrent modification, the iterator fails quickly and cleanly, rather than risking arbitrary, 
non-deterministic behavior at an undetermined time in the future.

Note that the fail-fast behavior of an iterator cannot be guaranteed as it is, generally speaking, impossible to make any hard guarantees in the presence of unsynchronized concurrent modification. 
Fail-fast iterators throw ConcurrentModificationException on a best-effort basis. Therefore, it would be wrong to write a program that depended on this exception for its correctness: the fail-fast 
behavior of iterators should be used only to detect bugs.


3. LINKED LISTS

LinkedList are composed of nodes and references or pointers from one node to another. The last reference pointing to null.

Node1 -> Node2 -> Node3 -> NULL

A single Node containing 2 important data.
1) Data -> Integer, Double .. Custom object.. etc.
2) Reference to next Node 


About Linked lists
1) Simple and very common Data structure
2) They can bu used to implement several other common data types such as Queue, Stack.
3) Simple Linked Lists by themself does not allow random access to the data. So we can use getItem(index).
4) Many basic operations such as obtaining the last node of list, finding node that contains a given data or locating the place where new node should be inserted - requires sequential scanning of
most or all of the list elements. 


PROS
1) LinkedList is dynamic data structure (Arrays are not!!!)
2) It can allocate needed memory in run-time
3) Very efficient if we want to manupulate the first element
4) Easy implementation
5) Can store items with different size. (And array assumes every item are with same type)
6) It is easier for LinkedList to grow organically. An array's size to be needed in compile time or recrate when it needs to grow.

CONS
1) Waste of memory because of references to next node
2) Nodes in the list should be read from beginning and that is why random access will NOT run in O(1) time like an array.
3) Single linkedLists are extremly difficult to for reverse travering (To read backward or iterate backward). Because nodes does not contain refernces to previous node. Solution is Double Linked List. But
disadvantages is more waste of memory because this time every node contains 2 references - one to previous and second to next node.



SINGLE LINKEDLIST OPERATIONS AND THEIR TIME COMPLEXITIES
1) Retreving first item of list. O(1)
2) Retreving item at a given index. O(N) 

3) Removing first item of list. O(1)
4) Removing an item at a given index. O(N)


LINKEDLIST IN JAVA

Doubly-linked list implementation of the List and Deque interfaces. Implements all optional list operations, and permits all elements (including null).
All of the operations perform as could be expected for a doubly-linked list. Operations that index into the list will traverse the list from the beginning or the end, whichever is closer to the specified index.

Note that this implementation is not synchronized. If multiple threads access a linked list concurrently, and at least one of the threads modifies the list structurally, it must be synchronized externally. 
(A structural modification is any operation that adds or deletes one or more elements; merely setting the value of an element is not a structural modification.) This is typically accomplished by 
synchronizing on some object that naturally encapsulates the list. If no such object exists, the list should be "wrapped" using the Collections.synchronizedList method. This is best done at creation time, 
to prevent accidental unsynchronized access to the list:

   List list = Collections.synchronizedList(new LinkedList(...));

The iterators returned by this class's iterator and listIterator methods are fail-fast: if the list is structurally modified at any time after the iterator is created, in any way except through the 
Iterator's own remove or add methods, the iterator will throw a ConcurrentModificationException. Thus, in the face of concurrent modification, the iterator fails quickly and cleanly, rather than 
risking arbitrary, non-deterministic behavior at an undetermined time in the future.

Note that the fail-fast behavior of an iterator cannot be guaranteed as it is, generally speaking, impossible to make any hard guarantees in the presence of unsynchronized concurrent modification. 
Fail-fast iterators throw ConcurrentModificationException on a best-effort basis. Therefore, it would be wrong to write a program that depended on this exception for its correctness: the fail-fast 
behavior of iterators should be used only to detect bugs.

This class is a member of the Java Collections Framework.


3. STACKS

In computer science, a stack is an abstract data type that serves as a collection of elements, with two main principal operations:

- Push, which adds an element to the collection, and
- Pop, which removes the most recently added element that was not yet removed.

Basic operations are 
- peek() - returning the last inserted item from stack
- pop() - removing the last inserted item from stack
- push() - adding new item to stack

Stack has LIFO (Last in First out) structure.
In most high level languagaes stack can be implemented easyly with array or linked list.


STACK MEMORY
- The most important application of stack abstract data type. 
- It is a special region of memory in the RAM. 
- The details of how to store data in stack are normally hidden in high-level programming languages.
- It keeps track of point to which each active subrutine should return control when it finishes executing. 
- Stores temporaly variables created by each fundtion.
- Stack memory is limited
- Every time local variables are created they are pushed into stack memory.Once method ends those variables will be gone from stack memory.

HEAP MEMORY
- The region of the meomory that is not managed automatically for you.
- This is large part of memory. (Unlike stack)
- In JAVA references types and objects are stored in heap memory
- We have to deallocate the chunks because heap memory is not managed autamitaclly. But in java there is also garbage collector which helps us in this sense
- If we would not remove chunks from heap and not manage it correctly it will lead us to MEMORY LEAK.
- Heap memory is slower than Stack


DIFFERENCES OF STACK AND HEAP
HEAP
- no size limit
- slow access
- objects and references types will be stored here
- we should manage memory and remove chunks

STACK
- size limit
- fast access
- local variables are stored
- space efficienlt managed by CPU


STACK IN JAVA
public class Stack<E>
extends Vector<E>
The Stack class represents a last-in-first-out (LIFO) stack of objects. It extends class Vector with five operations that allow a vector to be treated as a stack. The usual push and pop operations are 
provided, as well as a method to peek at the top item on the stack, a method to test for whether the stack is empty, and a method to search the stack for an item and discover how far it is from the top.
When a stack is first created, it contains no items.

A more complete and consistent set of LIFO stack operations is provided by the Deque interface and its implementations, which should be used in preference to this class. For example:

   
   Deque<Integer> stack = new ArrayDeque<Integer>();

Since:
JAVA 1.0

3. Queues

In computer science, a queue is a collection of entities that are maintained in a sequence and can be modified by the addition of entities at one end of the sequence and the removal of entities from the
other end of the sequence.

- Abstract data type
- Fifo structure
- Basci operations are, dequeue(), enqueue(), peek();
- It can be implemented with help of dynamic array and linked list

Operations
- enqueue(Object o) adding an item at the end of the queue
- dequeue() removing an item at top of the queue
- peek() retrieving an item at the top of the queue.

Most popular applications
- when resource is shared by multiply consumers (threads) we store them in queue. 
- For example CPU Scheduling
- when data is trasferred asynchrously 
- For example: IO Buffers


Queue in JDK
https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/Queue.html
https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/PriorityQueue.html


4. Binary Search Trees (BST)

In computer science, a binary search tree (BST), also called an ordered or sorted binary tree, is a rooted binary tree whose internal nodes each store a key greater than all the keys in the node's 
left subtree and less than those in its right subtree. A binary tree is a type of data structure for storing data such as numbers in an organized way. Binary search trees allow binary search for 
fast lookup, addition and removal of data items, and can be used to implement dynamic sets and lookup tables. The order of nodes in a BST means that each comparison skips about half of the remaining tree, 
so the whole lookup takes time proportional to the binary logarithm of the number of items stored in the tree. This is much better than the linear time required to find items by key in an (unsorted) array, 
but slower than the corresponding operations on hash tables.

In BST insertion and searching operation takes always O(logN) time complexity.

During the deletion of node from BST we might find ourself in 3 sort of different stiuation. Let's consider them one by one.

1) Node we wanna delete is a leaf node. Leaf node of tree is the node which does not have any child node. So in this case our job is gonna be very easy. Basically we will just set that node to null and that is all. Time 
complexity of this is gonna be O(logN) .
2) The node we wanna delete might have only 1 child. Does not matter on the right or left. It is also very easy job for us. We will replace this node with its that only child. And that is all. It will
also reqire O(logN) .
3) The node we wanna delete has 2 child. This is a little bit complex stiuation for us. There are several ways to handle this. One of the easiest way is below:
Just after finding the node we wanna remove will either the bigest node in the left subtree of that or smallest node in the right subtree of that and swap them. After that we will absolutely have one of the stiuation above for the node
we wanted delete. And we know how to handle them. So it will also take O(logN) time complexity.


Traversal of BST
The method to chouse to traverse the BST might be important. There some common ways to do it.
1) In-order traversal. With this method we start from left node then go to root then go to right recursively. It will give us sorted traversal.
2) Pre-order traversal. With this method we visit root then go to left then right recursively.
3) Post-order traversal. With this method we first visit left then go to right then root recursively.

Generally if the tree becomes very unbalances (for example we create a treee from sorted arrays then it will have only right childs) all the O(logN) operations are going to closer to O(N) linear time complexity. That is why
it is very important to keep tree as much as possible balanced.

5. BALANCED TREE AVL TREE




























